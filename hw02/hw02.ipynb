{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biostat 257 Homework 2\n",
    "\n",
    "**Due Apr 30 @ 11:59PM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versioninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a linear mixed effects model\n",
    "$$\n",
    "    \\mathbf{Y}_i = \\mathbf{X}_i \\boldsymbol{\\beta} + \\mathbf{Z}_i \\boldsymbol{\\gamma} + \\boldsymbol{\\epsilon}_i, \\quad i=1,\\ldots,n,\n",
    "$$\n",
    "where   \n",
    "- $\\mathbf{Y}_i \\in \\mathbb{R}^{n_i}$ is the response vector of $i$-th individual,  \n",
    "- $\\mathbf{X}_i \\in \\mathbb{R}^{n_i \\times p}$ is the fixed effect predictor matrix of $i$-th individual,  \n",
    "- $\\mathbf{Z}_i \\in \\mathbb{R}^{n_i \\times q}$ is the random effect predictor matrix of $i$-th individual,  \n",
    "- $\\boldsymbol{\\epsilon}_i \\in \\mathbb{R}^{n_i}$ are multivariate normal $N(\\mathbf{0}_{n_i},\\sigma^2 \\mathbf{I}_{n_i})$,  \n",
    "- $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ are fixed effects, and  \n",
    "- $\\boldsymbol{\\gamma} \\in \\mathbb{R}^q$ are random effects assumed to be $N(\\mathbf{0}_q, \\boldsymbol{\\Sigma}_{q \\times q}$) independent of $\\boldsymbol{\\epsilon}_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 Formula (10 pts)\n",
    "\n",
    "Write down the log-likelihood of the $i$-th datum $(\\mathbf{y}_i, \\mathbf{X}_i, \\mathbf{Z}_i)$ given parameters $(\\boldsymbol{\\beta}, \\boldsymbol{\\Sigma}, \\sigma^2)$. \n",
    "\n",
    "**Hint:** For non-statisticians, feel free to ask for help in class or office hours. Point of this exercise is computing not statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Var(Y) = \\mathbf{Z_i \\Sigma Z_i^T} + \\sigma^2 \\mathbf{I}$$\n",
    "$$l(\\beta, \\Sigma, \\sigma^2) = -\\frac{1}{2}\\ln|2\\pi(\\mathbf{Z_i \\Sigma Z_i^T} + \\sigma^2 \\mathbf{I_{n_i}})| - \\frac{1}{2}(\\mathbf{y_i} - \\mathbf{X_i\\beta})^T[\\mathbf{Z_i \\Sigma Z_i^T} + \\sigma^2 \\mathbf{I_{n_i}}]^{-1}(\\mathbf{y_i} - \\mathbf{X_i\\beta})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We haven't covered log-likelihood functions in 200C / for mixed linear models, so I didn't provide a derivation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 Start-up code\n",
    "\n",
    "Use the following template to define a type `LmmObs` that holds an LMM datum $(\\mathbf{y}_i, \\mathbf{X}_i, \\mathbf{Z}_i)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woodbury Identity:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$[\\mathbf{Z_i \\Sigma Z_i^T} + \\sigma^2 \\mathbf{I_{n_i}}]^{-1} $$\n",
    "$$= [\\mathbf{(Z_i L) (Z_i L)^T} + \\sigma^2 \\mathbf{I_{n_i}}]^{-1}$$\n",
    "$$ = [\\sigma^2 \\mathbf{I_{n_i}}]^{-1} - [\\sigma^2 \\mathbf{I_{n_i}}]^{-1}\\mathbf{Z_iL}[I + (\\mathbf{(Z_iL)^T}(\\sigma^2 \\mathbf{I_{n_i}})^{-1}\\mathbf{(Z_iL)^T})^{-1}]\\mathbf{(Z_iL)^T}[\\sigma^2 \\mathbf{I_{n_i}}]^{-1}$$\n",
    "$$ = \\frac{1}{\\sigma^2}\\mathbf{I} -\\frac{1}{\\sigma^4}\\mathbf{Z_iL}[\\mathbf{I} + \\frac{1}{\\sigma^2}(\\mathbf{Z_iL})^T(\\mathbf{Z_iL})]^{-1}(\\mathbf{Z_iL})^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our term $(\\mathbf{Z_iL})^T(\\mathbf{Z_iL})$ is symmetric and positive definite. Adding it to the identity matrix maintains this property. Thus, Cholesky decomposition may be applied:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbf{I} + \\frac{1}{\\sigma^2}(\\mathbf{Z_iL})^T(\\mathbf{Z_iL}) = \\mathbf{AA^T} $$\n",
    "$$ [\\mathbf{Z_i \\Sigma Z_i^T} + \\sigma^2 \\mathbf{I_{n_i}}]^{-1} = \\frac{1}{\\sigma^2}\\mathbf{I} - \\frac{1}{\\sigma^4}\\mathbf{Z_iL}(\\mathbf{AA^T})^{-1}(\\mathbf{Z_iL})^T $$\n",
    "$$ =\\frac{1}{\\sigma^2}\\mathbf{I} - \\frac{1}{\\sigma^4}\\mathbf{Z_iL}\\mathbf{A^{-T}}\\mathbf{A^{-1}}(\\mathbf{Z_iL})^T $$\n",
    "$$ = \\frac{1}{\\sigma^2}\\mathbf{I} - \\frac{1}{\\sigma^4}(\\mathbf{A^{-1}(Z_iL)^T})^T(\\mathbf{A^{-1}(Z_iL)^T})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the determinant term using $\\text{det}(\\mathbf{A} + \\mathbf{U} \\mathbf{V}^T) = \\text{det}(\\mathbf{A}) \\text{det}(\\mathbf{I}_m + \\mathbf{V}^T \\mathbf{A}^{-1} \\mathbf{U})$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$-\\frac{1}{2}\\ln|2\\pi(\\mathbf{Z_i \\Sigma Z_i^T} + \\sigma^2 \\mathbf{I_{n_i}})| = -\\frac{1}{2}\\ln(2\\pi^{n_i}) -\\frac{1}{2}\\ln[|\\mathbf{(Z_i L) (Z_i L)^T} + \\sigma^2 \\mathbf{I_{n_i}}|]$$\n",
    "$$ = -\\frac{1}{2}\\ln(2\\pi^n) - \\frac{1}{2}\\ln[|\\sigma^2\\mathbf{I_{n_i}}||\\mathbf{I} + \\frac{1}{\\sigma^2}(\\mathbf{Z_iL})^T(\\mathbf{Z_iL})|]$$\n",
    "$$ = -\\frac{n}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln((\\sigma^2)^n) - \\frac{1}{2}\\ln|\\mathbf{AA^T}| $$\n",
    "$$ = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2}\\ln|\\mathbf{AA^T}| $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final simplification:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ l(\\beta, \\Sigma, \\sigma^2) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2}\\ln|\\mathbf{AA^T}| - \\frac{1}{2}(\\mathbf{y_i} - \\mathbf{X_i\\beta})^T[\\frac{1}{\\sigma^2}\\mathbf{I} - \\frac{1}{\\sigma^4}(\\mathbf{A^{-1}(Z_iL)^T})^T(\\mathbf{A^{-1}(Z_iL)^T})](\\mathbf{y_i} - \\mathbf{X_i\\beta})$$\n",
    "$$ = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2}\\ln|\\mathbf{AA^T}| - \\frac{1}{2\\sigma^2}(\\mathbf{y_i} - \\mathbf{X_i\\beta})^T(\\mathbf{y_i} - \\mathbf{X_i\\beta}) + \\frac{1}{2\\sigma^4}(\\mathbf{y_i} - \\mathbf{X_i\\beta})^T(\\mathbf{A^{-1}(Z_iL)^T})^T(\\mathbf{A^{-1}(Z_iL)^T})(\\mathbf{y_i} - \\mathbf{X_i\\beta})$$\n",
    "$$ = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2}\\ln|\\mathbf{AA^T}| - \\frac{1}{2\\sigma^2}(\\mathbf{y_i} - \\mathbf{X_i\\beta})^T(\\mathbf{y_i} - \\mathbf{X_i\\beta}) + \\frac{1}{2\\sigma^4}[(\\mathbf{A^{-1}(Z_iL)^T})(\\mathbf{y_i} - \\mathbf{X_i\\beta})]^T[(\\mathbf{A^{-1}(Z_iL)^T})(\\mathbf{y_i} - \\mathbf{X_i\\beta})]$$\n",
    "$$ = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2}\\ln|\\mathbf{AA^T}| - \\frac{1}{2\\sigma^2}(\\mathbf{y^Ty} - 2\\mathbf{y^TX\\beta} + \\mathbf{\\beta^TX^TX\\beta)} + \\frac{1}{2\\sigma^4}[(\\mathbf{A^{-1}L^T})(\\mathbf{Z^TY - Z^TX\\beta)}]^T[(\\mathbf{A^{-1}L^T})(\\mathbf{Z^TY - Z^TX\\beta)}]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function, with interface   \n",
    "```julia\n",
    "logl!(obs, β, L, σ²)\n",
    "```\n",
    "that evaluates the log-likelihood of the $i$-th datum. Here `L` is the lower triangular Cholesky factor from the Cholesky decomposition `Σ=LL'`. Make your code efficient in the $n_i \\gg q$ case. Think the intensive longitudinal measurement setting.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LmmObs"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct LmmObs{T <: AbstractFloat}\n",
    "    # data\n",
    "    y :: Vector{T}\n",
    "    X :: Matrix{T}\n",
    "    Z :: Matrix{T}\n",
    "    # working arrays\n",
    "    # whatever intermediate arrays you may want to pre-allocate\n",
    "    res :: Vector{T}\n",
    "    det :: Matrix{T}\n",
    "    det1 :: Matrix{T}\n",
    "    det2 :: Matrix{T} \n",
    "    ztz :: Matrix{T}\n",
    "    yty :: T\n",
    "    xty :: Vector{T}\n",
    "    xtx :: Matrix{T}\n",
    "    ztx :: Matrix{T}\n",
    "    zty :: Vector{T}\n",
    "    last :: Vector{T}\n",
    "end\n",
    "\n",
    "# constructor\n",
    "function LmmObs(\n",
    "        y::Vector{T}, \n",
    "        X::Matrix{T}, \n",
    "        Z::Matrix{T}) where T <: AbstractFloat\n",
    "    res = similar(y)\n",
    "    ztz = transpose(Z) * Z\n",
    "    det = similar(ztz)\n",
    "    det1 = similar(ztz)\n",
    "    det2 = similar(ztz)\n",
    "    yty = transpose(y) * y\n",
    "    xty = transpose(X) * y\n",
    "    xtx = transpose(X) * X\n",
    "    ztx = transpose(Z) * X\n",
    "    zty = transpose(Z) * y\n",
    "    last = Vector{T}(undef, size(Z, 2))\n",
    "    \n",
    "    LmmObs(y, X, Z, res, det, det1, det2, ztz, yty, xty, xtx, ztx, zty, last)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logl! (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "function logl!(\n",
    "        obs :: LmmObs{T}, \n",
    "        β   :: Vector{T}, \n",
    "        L   :: Matrix{T}, \n",
    "        σ²  :: T) where T <: AbstractFloat\n",
    "    n, p, q = size(obs.X, 1), size(obs.X, 2), size(obs.Z, 2) \n",
    "    \n",
    "    #compute determinant term \n",
    "    mul!(obs.det, L', obs.ztz)\n",
    "    mul!(obs.det1, obs.det, L)\n",
    "    mul!(obs.det2, obs.det1, 1/σ²)\n",
    "    \n",
    "    iden = 1 * Matrix(I, size(obs.det2, 1), size(obs.det2, 1))\n",
    "    axpy!(1, iden, obs.det2)\n",
    "    \n",
    "    chol = cholesky!(Symmetric(obs.det2)) \n",
    "        \n",
    "    #compute last term\n",
    "    mul!(obs.last, obs.ztx, β)\n",
    "    axpby!(1, obs.zty, -1, obs.last)\n",
    "    \n",
    "    return -n//2 * log(2π) - n//2 * log(σ²) - 1//2 * logdet(chol) - \n",
    "        1 / (2 * σ²) * (obs.yty - 2 * obs.xty' * β + β' * obs.xtx * β)  + \n",
    "        1 / (2 * σ²^2) * dot(L' * obs.last, chol \\ L' * obs.last)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**: This function shouldn't be very long. Mine, obeying 80-character rule, is 25 lines. If you find yourself writing very long code, you're on the wrong track. Think about algorithm first then use BLAS functions to reduce memory allocations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 Correctness (15 pts)\n",
    "\n",
    "Compare your result (both accuracy and timing) to the [Distributions.jl](https://juliastats.org/Distributions.jl/stable/multivariate/#Distributions.AbstractMvNormal) package using following data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LmmObs{Float64}([5.982282543893974, 2.1686591674794546, -0.600617976847127, -2.5921722427552263, -4.1950904215114395, 0.19644359912941595, -1.8584206360155058, -0.471394614388984, 3.8132743617962293, 2.70502980503809  …  -0.9698126660920986, -0.056282731487944146, 0.1361331008072202, -0.27281934708288147, 3.204079310642381, 3.1301513446894242, -0.7337423498389697, -1.1992972996496185, 1.552593065317924, 0.9964276305957047], [1.0 -2.506566300781151 … -0.18291088048140966 0.4598620195142903; 1.0 -0.974090320735282 … -0.7383659530381397 0.4874285643091131; … ; 1.0 0.3528183431516365 … 1.9292747611484156 1.2067313494938754; 1.0 0.019416493632924994 … -1.3213407131023014 -0.06848691909471624], [1.0 0.8585392934004743 -1.9174716700008398; 1.0 0.9654277213047047 0.40862623314285934; … ; 1.0 -0.2387107330196111 -2.0711110232845926; 1.0 -0.9537172982914403 -1.1799476703506613], [NaN, 9.20483112e-315, NaN, 8.487983165e-314, 9.20483428e-315, 9.20482798e-315, NaN, NaN, 9.18964386e-315, 9.870701296e-315  …  2.1219957905e-314, 0.0, 0.0, 0.0, 0.0, 9.870779793e-315, 8.487983164e-314, 0.0, 0.0, 0.0], [9.57757116e-315 9.57757464e-315 9.69562352e-315; 9.695619253e-315 9.577575033e-315 9.577575824e-315; 9.57757385e-315 9.57757543e-315 9.577573057e-315], [9.577578196e-315 9.577580567e-315 9.695631585e-315; 9.695630636e-315 9.57758096e-315 9.577581753e-315; 9.57758017e-315 9.57758136e-315 9.577579777e-315], [9.577584124e-315 9.5775861e-315 9.6956368e-315; 9.695635854e-315 9.577586496e-315 9.577587286e-315; 9.577585705e-315 9.57758689e-315 9.57758531e-315], [2000.0 -5.306938396090018 -4.408376009219221; -5.306938396090018 2047.0348365544392 29.310444065639146; -4.408376009219221 29.310444065639146 2046.2572130622634], 7920.5925159675935, [1501.2716973619506, -2008.2435847529, 284.16176166173335, 844.4520000008704, 262.32459555580766], [2000.0 7.318286021569559 … -27.560057241455503 19.2754540958741; 7.318286021569559 2010.578706518849 … -5.879336208071432 32.624374120251645; … ; -27.560057241455503 -5.879336208071432 … 1982.3343610649044 46.732854634090415; 19.2754540958741 32.624374120251645 … 46.732854634090415 1901.632325790154], [2000.0 7.318286021569559 … -27.560057241455503 19.275454095874103; -5.306938396090018 -42.92812049984631 … -28.34862491789308 6.576275318435949; -4.408376009219221 -19.119818852719604 … -19.657259367593916 50.56755950821937], [1501.2716973619504, -30.875172466163445, -1614.2450569926816], [1.629135144e-315, 1.156284914e-315, 1.31553144e-315])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using BenchmarkTools, Distributions, LinearAlgebra, Random\n",
    "\n",
    "Random.seed!(257)\n",
    "# dimension\n",
    "n, p, q = 2000, 5, 3\n",
    "# predictors\n",
    "X  = [ones(n) randn(n, p - 1)]\n",
    "Z  = [ones(n) randn(n, q - 1)]\n",
    "# parameter values\n",
    "β  = [2.0; -1.0; rand(p - 2)]\n",
    "σ² = 1.5\n",
    "Σ  = fill(0.1, q, q) + 0.9I\n",
    "# generate y\n",
    "y  = X * β + Z * rand(MvNormal(Σ)) + sqrt(σ²) * randn(n)\n",
    "\n",
    "# form an LmmObs object\n",
    "obs = LmmObs(y, X, Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the standard way to evaluate log-density of a multivariate normal, using the Distributions.jl package. Let's evaluate the log-likelihood of this datum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3261.9177559187597"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "μ  = X * β\n",
    "Ω  = Z * Σ * transpose(Z) +  σ² * I\n",
    "mvn = MvNormal(μ, Symmetric(Ω)) # MVN(μ, Σ)\n",
    "logpdf(mvn, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that your answer matches that from Distributions.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3261.91775591876"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = Matrix(cholesky(Σ).L)\n",
    "logl!(obs, β, L, σ²)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You will lose all 15 + 30 + 30 = 75 points** if the following statement throws `AssertionError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@assert logl!(obs, β, Matrix(cholesky(Σ).L), σ²) ≈ logpdf(mvn, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 Efficiency (30 pts)\n",
    "\n",
    "Benchmarking your code and compare to the Distributions.jl function `logpdf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  30.55 MiB\n",
       "  allocs estimate:  5\n",
       "  --------------\n",
       "  minimum time:     14.158 ms (0.00% GC)\n",
       "  median time:      16.510 ms (0.00% GC)\n",
       "  mean time:        18.138 ms (13.86% GC)\n",
       "  maximum time:     28.277 ms (33.95% GC)\n",
       "  --------------\n",
       "  samples:          276\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# benchmark the `logpdf` function in Distribution.jl\n",
    "bm1 = @benchmark logpdf($mvn, $y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  944 bytes\n",
       "  allocs estimate:  9\n",
       "  --------------\n",
       "  minimum time:     1.180 μs (0.00% GC)\n",
       "  median time:      1.510 μs (0.00% GC)\n",
       "  mean time:        1.543 μs (2.27% GC)\n",
       "  maximum time:     353.940 μs (98.79% GC)\n",
       "  --------------\n",
       "  samples:          10000\n",
       "  evals/sample:     10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# benchmark your implementation\n",
    "L = Matrix(cholesky(Σ).L)\n",
    "bm2 = @benchmark logl!($obs, $β, $L, $σ²)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The points you will get is\n",
    "$$\n",
    "\\frac{x}{10000} \\times 30,\n",
    "$$\n",
    "where $x$ is the speedup of your program against the standard method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the points you'll get\n",
    "clamp(median(bm1).time / median(bm2).time / 10000 * 30, 0, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**: Apparently I am using 10000 as denominator because I expect your code to be at least $10000 \\times$ faster than the standard method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5 Memory (30 pts)\n",
    "\n",
    "You want to avoid memory allocation in the \"hot\" function `logl!`. You will lose 1 point for each `1 KiB = 1024 bytes` memory allocation. In other words, the points you get for this question is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.078125"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clamp(30 - median(bm2).memory / 1024, 0, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**: I am able to reduce the memory allocation to 0 bytes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6 Misc (15 pts)\n",
    "\n",
    "Coding style, Git workflow, etc. For reproducibity, make sure we (TA and myself) can run your Jupyter Notebook. That is how we grade Q4 and Q5. If we cannot run it, you will get zero points."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "87px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
